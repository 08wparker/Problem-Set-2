---
title: "PS 2"
author: "William Parker"
subtitle: Unsupervised Machine Learning (40800)
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
    toc_depth: '2'
  html_notebook:
    toc: yes
    toc_depth: 2
    toc_float: yes
---

```{r libraries, message= FALSE, warning = FALSE}
library(tidyverse)
library(dendextend)
```


# Computation

**You fielded a survey and collected some wildly descriptive feature vectors. Use the following vectors to address questions 1-3:**

```{r vectors}
p <- c(1,2)

q <-c(3,4)
```


## 1. Calculate Manhattan, Canberra, and Euclidean distances “by hand” (i.e., create the data, program each line, and make the calculations). What are the values for each measure?

### Manhattan 

$d_{manhattan}(p,q) = \sum_{i = 1}^n |p_i - q_i|$

```{r manhattan}

d_minkowski <- function(p, q, m){
  
  dist <- function(x,y) abs(x-y)^m
  
  (sum(mapply(dist, p, q)))^(1/m)
}

Manhattan_dist <- d_minkowski(p, q, 1)

```

the manhattan distance is `r Manhattan_dist`


### Canberra


$d_{manhattan}(p,q) = \sum_{i = 1}^n \frac{|p_i - q_i|}{|p_i| + |q_i|}$

```{r canberra}

d_canberra <- function(p, q){
  
  dist <- function(x,y) abs(x-y)/(abs(x) + abs(y))
  
  sum(mapply(dist, p, q))
}

Canberra_dist <- d_canberra(p,q)
```

The Canberra distance is `r Canberra_dist`


### Euclidean

$d_{Euclidean} = \sqrt{\sum_{i = 1}^n (p_i - q_i)^2}$


```{r euclidean}
Euclidean_dist <- d_minkowski(p, q, 2)
```

The Euclidean distance is `r Euclidean_dist`


## 2. Use the dist() function in R to check your work. Were you right or wrong? (be honest in your reporting). If wrong, after debugging, where and why did you go wrong?


```{r check_manhattan}
dist(rbind(p,q), method = "manhattan")
```


```{r check_canberra}
dist(rbind(p,q), method = "canberra")
```

```{r check_euclidean}
dist(rbind(p,q), method = "euclidean")
```
All my calculated distances were correct!

## 3. What are the key differences between these measures, and why does it matter? How might you see these differences “in action” with these fictitious data?

The Manhattan distance is the sum of the absolute values of the differences between each vector component, here it is larger than the euclidean distance which is the square root of the sum of square of distances between components. If p and q were physical points in space this would represent the "taxicab" and "crow files" distances respectively, and explain why the manhattan distance is larger.

The Canberra is a weighted version of the manhattan distance. This would be more senstive to changes in p and q very close to the origin ((0,0) in this case), as it weighs each absolute value by the sum of the asbolute value of p and q


## 4. Use some basic EDA techniques to present and discuss the old faithful data set (e.g., visualize, describe in multiple ways, etc.)

I'll start by just looking at the dataset
```{r old_faithful}
faithful <- faithful

faithful
```

Looks like we have 2 variables, `eruptions` and `waiting`. I'll use the `skimr` package to get some numeric summaries of each variable

```{r skim}
skimr::skim(faithful)
```
We have a total of `r faithful %>% nrow()` observations of each variable. Reading the documentation, `waiting` is the time between each eruption event (? units) and `eruptions` is the duration of the eurption (in minutes). Regardless, both `waiting` and `eruptions` seem to have a bimodal distribution.

Next I do a simple scatter plot of the observations
```{r scatter}
faithful %>%
  ggplot(aes(x = waiting, y = eruptions)) +
  geom_point()+
  labs(title = "eruption time vs. waiting time")
```
Assuming that both scales are in minutes, we seem to have two clear clusters of eurption events. If the waiting time is shorter (50-65 min) then the eruption lasts around 2 minutes. If the waiting time if longer, the eruption last longer. While there is a clear positive correlation between waiting time and eruption duration, there is a paucity of data from 65-75 minutes, suggesting something is separating these two groups clearly.

From this plot I believe we can safely conclude that a linear model of data generation like $eruptionTime = \beta_0 + \beta_1*waiting + \epsilon$ is insufficient to fully describe what is going on.

I was curious to see how scaling (which we do in the next step) would effect the plot
```{r scaled_scatter}
faithful %>%
  scale() %>%
  as_tibble() %>%
  ggplot(aes(x = waiting, y = eruptions)) +
  geom_point() +
  labs(title = "scaled eruption time vs. scaled waiting time")
```
Looks exactly the same, which makes sens because `scale` just performs the same linear transformation on each axises (mean centers each column and divides each column value by it's standard deviation), which shouldn't effect the relative position of each point


## 5. Calculate a dissimilarity matrix of these data.

I will scale the data first with `scale` 
```{r dist_matrix}
faith_dist <- faithful %>%
  scale() %>%
  dist()

length(faith_dist)
```
I calculate the distance matrix which is `r faithful %>% nrow()` x `r faithful %>% nrow()`. This matrix should have `r (faithful %>% nrow())*(faithful %>% nrow())` entries, but the calculated object only has `r length(faith_dist)` entries. The explanaiton is for this is that the matrix is symmetric and all the diagonal elements are zero, so R only returns (`r (faithful %>% nrow())*(faithful %>% nrow())` - `r faithful %>% nrow()`)/2 = `r length(faith_dist)` important non-zero unique entries (i.e. $n*(n-1)/2$)

Note that I used euclidean distance which is the default distance metric in `dist()` 

## 6. Generate an ODI for the Old Faithful data. What do you see?
```{r ODI}
seriation::dissplot(faith_dist)
```

The Ordered Dissimilarity Image (ODI) demonstrates two clear clusters of observations.


## 7. Using any munging tools you’d like (e.g., dplyr from the Tidyverse), create a subset of the `iris ` data excluding the species feature

```{r munge_iris}
iris_no_species <- iris %>% 
  select(-Species)

  
iris_no_species
```

scale the features and calculate a dissimilarity matrix
```{r dist_matrix_iris}
iris_dist <- iris_no_species %>%
  scale() %>%
  dist()

length(iris_dist)
```

We get the expected $n*(n-1)/2$ number of unique non-zero entries

## 8. Fit an agglomerative hierarchical clustering algorithm using complete linkage on your subset data and render the dendrogram of clustering results. What do you see?

```{r dendrogram}

hc_complete <- hclust(iris_dist, 
                      method = "complete")


ggdendro::dendro_data(hc_complete)$segments %>%
  ggplot() +
  geom_segment(aes(x = x, y = y, xend = xend, yend = yend)) +
  labs(y = "distance", x = "flower index")
```

early on the tree (distance just below 6) could be cut at $k = 3$.  Another natural cut would be just below a distance of 4, yielding $k = 5$.


## 9. Try cutting the tree at 2 and 3 branches and show these trees side-by-side. How do they differ?

```{r cuts}
cuts <- cutree(hc_complete, 
               k = c(2,3))


table(`2 Clusters` = cuts[,1], 
      `3 Clusters` = cuts[,2])
```

Looks like Cluster 1 in the $k =2$ cut gets divided pretty significantly into clusters of 49 and 24 observations. Cluster 2 in the $k=2$ cut stays intact (becoming cluster #3 in the $k=3$ cut). this suggests that 3 cuts are probably better than 2.

To plot the cuts with color, I use the `dendextend` package

```{r side_by_side_plot, warning = FALSE, message = FALSE}
dend <-  iris_no_species %>%
  scale() %>%
  dist() %>%
  hclust(method = "complete") %>%
  as.dendrogram()

par(mfrow = c(1, 2))

dend %>% 
  set("branches_k_color",  value = c("darkgreen", "purple"), k = 2)  %>%
  set("labels", NA ) %>%
  plot(main = "k = 2")

dend %>% 
  set("branches_k_color",  value = c("darkgreen", "red", "darkblue"), k = 3)  %>%
  set("labels", NA ) %>%
  plot(main = "k = 3")
```

## 10. Now fit the algorithm using single and complete linkage and present each dendrogram side-by-side. Discuss the differences. What effects can we see in the clustering patterns when using different linkage methods?

```{r side_by_side_linkage, warning = FALSE, message= FALSE}
par(mfrow = c(1, 2))

iris_no_species %>%
  scale() %>%
  dist() %>%
  hclust(method = "complete") %>%
  as.dendrogram() %>%
  set("labels", NA ) %>%
  plot(main = "complete")
  
iris_no_species %>%
  scale() %>%
  dist() %>%
  hclust(method = "single") %>%
  as.dendrogram() %>%
  set("labels", NA ) %>%
  plot(main = "single")
```
The differences between the complete and single methods are quite striking here. the complete method joins the two clusters with the smallest distance between their elements that are furthest apart, whereas the single method joins the two clusters that have the shortest distance between their closest elements. 

Thus the single method is more likely to "chain" mainy roups with the same distance from single elements being close together, even though the groups may be far apart. Complete clustering finds more compact clusters of similar diameter, that is apparent in this example as well.

Interestingly despite the differing tree structure, the $k = 3$ cut groups seem to be relatively preserved, although the first group has more observations now

# Critical Thinking
## 1. You just assessed the clusterability of some feature space $\rm I\!R^n$. Address the following questions:

### a. How would you go about determining whether clustering made sense to consider or not?

Before launching into techniques designed to diagnose clusterability (see my answer to b below), I would think about the nature of the data and possible models of the data generating process and ask

1. based on what we know about where the data came from, is it reasonable to suspect that the data may be clustered? 
2. if the data are clustered, so what? What does the clustering add to our knowledge about the data?

To use a medical example, let's say there is reason to suspect disease X might not just be one distinct disease but instead multiple similar disease states with different causes and treatments. Proceeding to diagnosing clusterability in a high dimensional clinical dataset of patient with disease X in this context makes a lot of sense.


### b. What are techniques you would use, and what might you be looking for from each?

1. exploratory data analysis (EDA). Like in the old faithful example, a grid of scatter plots of different combinations of features may demonstrate a clear case for clustering. If the dimension $n$ is very high, I would use a dimension reduction technique like principle component analysis and then perform scatter plots of the first two PCA components.
2. generate a distance matrix and then create a visual assessment of cluster tendency like the ordered dissimilarity matrix
3. Use the same distance matrix to calculate a hopkin's statistic and test the null hypothesis of spatial randomness

### c. How might these techniques work together to motivate clustering or not?

If EDA is highly suggestive of clustering, then ODI and the hopkin's statistic would just be confirmatory. However if EDA does not reveal a clear clustering pattern, then ODI and hopkin's statistic may suggest clustering where it was not readily apparent from the EDA. 


### d. And ultimately, can/should you proceed if you find little to no support for clusterability? Why or why not?

I think proceeding would depend on your prior belief that clusters could be involved in the data generation process. If it was strong enough I would proceed regardless of the results of the diagnostic tests for clusterability, especially if the results of clustering could answer a clear research hypothesis.

On the other hand, if I was skeptical about the clusterability of the data based on what was known about the data generating process, negative diagnostics for clusterability would convince me to stop.


## 2. Locate (and read) a paper that applies the hierarchical agglomerative clustering technique. Address the following questions:

### a. Describe the author(s) process.

I read Johnson, S. C. (1967). "Hierarchical clustering schemes." Psychometrika, 32(3), 241-254. Tis seems like a foundational paper in the hierarchial agglomerative clustering field. the authors set out to formally define the mechanics of agglomerative clustering and work through an example in painstanking detail to demonstrate them. They specifically demonstrate the complete (maximum) and single (minimum) linkage methods. 

### b. Do they go through similar steps as we covered this week both in setting the stage for clustering (e.g., assessing clusterability, calculating distance, etc.), as well as in fitting the algorithm? If not, what did they omit and does this omission impact their findings in your opinion?

they did not appear to do any clusterability diagnositics, but did follow similar steps after that point including

1. create a distance matrix using all $n*(n-1)/2$ similarity measures
2. initiate with singletons
3. form a cluster according to linkage method (either single or complete in their case)
4. record distance between clusters on linkage method scale
5. repeat steps 3-4 until only one cluster with all the observations exists

They omit the other linkage methods we learned about, as they think the single = "connectedness" and complete = "compactness" have the clearest interpreations.

### c. Describe at least one possible extension from the study that could emerge based on their findings.

The authors show the distance metric used in the agglomerative hierarchial cluster scheme linkage method need to statisfy not just the standard triangle inequality $d(x,z) ≤ d(x, y) + d(y,z)$ but the stronger "ultrametric inequality" that for 3 points $x,y,z$ in clusters $x, y \in C_i$ and $y, z \in C_j$ then $d(x,z) ≤ \max{[d(x,y), d(y,z)]}$. They suggest an unsolved problem is finding the "closest" metric that statisfies the ultrametric inequality.
